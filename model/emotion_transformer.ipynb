{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b437e0f-9da8-40ee-9285-00bd5270fe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46bf118e-1d77-4c06-8cd7-78e9a4bfc926",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarkDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "344907b9-30a8-4055-b706-cede81be0a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionTransformer(nn.Module):\n",
    "    def __init__(self, input_dim=3, seq_len=468, model_dim=64, num_heads=4, num_classes=6):\n",
    "        super(EmotionTransformer, self).__init__()\n",
    "        self.input_fc = nn.Linear(input_dim, model_dim)\n",
    "        self.ln1 = nn.LayerNorm(model_dim)  # LayerNorm to stabilize input to Transformer\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=model_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=128,\n",
    "            dropout=0.1,\n",
    "            batch_first=True,\n",
    "            activation=\"gelu\",  # more stable than ReLU for Transformers\n",
    "            norm_first=True     # stabilize early\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "\n",
    "        self.fc_out = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(model_dim * seq_len, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_fc(x)         # (B, 468, 3) â†’ (B, 468, model_dim)\n",
    "        x = self.ln1(x)              # Normalize before transformer\n",
    "        x = self.transformer_encoder(x)\n",
    "        out = self.fc_out(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dba9eb07-a36c-44f5-a417-039ccc2b65f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, device, epochs=10):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs} ----------------------\")\n",
    "\n",
    "        for x_batch, y_batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\", leave=False):\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_acc = (correct / total) * 100  # percentage\n",
    "\n",
    "        train_losses.append(avg_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "        val_acc = evaluate(model, val_loader, device)\n",
    "        val_accuracies.append(val_acc * 100)\n",
    "\n",
    "        print(f\"ðŸŸ¢ Training Loss: {avg_loss:.4f}\")\n",
    "        print(f\"âœ… Training Accuracy: {train_acc:.2f}%\")\n",
    "        print(f\"ðŸ”µ Validation Accuracy: {val_acc * 100:.2f}%\")\n",
    "    plot_metrics(train_losses, train_accuracies, val_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56f439c1-f119-4185-aba9-f101cb5b4b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in val_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            outputs = model(x_batch)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            y_true.extend(y_batch.numpy())\n",
    "            y_pred.extend(preds)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b791839-f947-40d2-b98e-b4e2be0f8b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sneha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10 ----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŸ¢ Training Loss: 7.3078\n",
      "âœ… Training Accuracy: 28.03%\n",
      "ðŸ”µ Validation Accuracy: 19.18%\n",
      "\n",
      "Epoch 2/10 ----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŸ¢ Training Loss: 2.7662\n",
      "âœ… Training Accuracy: 29.76%\n",
      "ðŸ”µ Validation Accuracy: 31.51%\n",
      "\n",
      "Epoch 3/10 ----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŸ¢ Training Loss: 1.8313\n",
      "âœ… Training Accuracy: 31.83%\n",
      "ðŸ”µ Validation Accuracy: 28.77%\n",
      "\n",
      "Epoch 4/10 ----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŸ¢ Training Loss: 1.6065\n",
      "âœ… Training Accuracy: 33.22%\n",
      "ðŸ”µ Validation Accuracy: 23.29%\n",
      "\n",
      "Epoch 5/10 ----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŸ¢ Training Loss: 1.7405\n",
      "âœ… Training Accuracy: 28.72%\n",
      "ðŸ”µ Validation Accuracy: 31.51%\n",
      "\n",
      "Epoch 6/10 ----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŸ¢ Training Loss: 1.6201\n",
      "âœ… Training Accuracy: 30.80%\n",
      "ðŸ”µ Validation Accuracy: 41.10%\n",
      "\n",
      "Epoch 7/10 ----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŸ¢ Training Loss: 1.6333\n",
      "âœ… Training Accuracy: 39.10%\n",
      "ðŸ”µ Validation Accuracy: 27.40%\n",
      "\n",
      "Epoch 8/10 ----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŸ¢ Training Loss: 1.6161\n",
      "âœ… Training Accuracy: 38.75%\n",
      "ðŸ”µ Validation Accuracy: 32.88%\n",
      "\n",
      "Epoch 9/10 ----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŸ¢ Training Loss: 1.5733\n",
      "âœ… Training Accuracy: 44.29%\n",
      "ðŸ”µ Validation Accuracy: 31.51%\n",
      "\n",
      "Epoch 10/10 ----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŸ¢ Training Loss: 1.5774\n",
      "âœ… Training Accuracy: 40.83%\n",
      "ðŸ”µ Validation Accuracy: 32.88%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plot_metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# ---------- Run ----------\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     model, le = \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC:\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mUsers\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mSneha\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mDownloads\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mJoyVerseDataSet.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(csv_path)\u001b[39m\n\u001b[32m     34\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m model = EmotionTransformer().to(device)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸŽ‰ Training completed. Model ready!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model, le\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, device, epochs)\u001b[39m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ… Training Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     43\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mðŸ”µ Validation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[43mplot_metrics\u001b[49m(train_losses, train_accuracies, val_accuracies)\n",
      "\u001b[31mNameError\u001b[39m: name 'plot_metrics' is not defined"
     ]
    }
   ],
   "source": [
    "def main(csv_path):\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Remove rows with missing or infinite values\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "    # Keep only labels that appear more than once\n",
    "    label_counts = df['emotion'].value_counts()\n",
    "    df = df[df['emotion'].isin(label_counts[label_counts > 1].index)]\n",
    "\n",
    "    # Select landmark columns (x_, y_, z_)\n",
    "    feature_cols = [col for col in df.columns if any(axis in col for axis in ['x_', 'y_', 'z_'])]\n",
    "\n",
    "    # Normalize features\n",
    "    features = df[feature_cols].values\n",
    "    features = (features - np.mean(features, axis=0)) / (np.std(features, axis=0) + 1e-6)\n",
    "\n",
    "    features = features.reshape(-1, 468, 3)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    labels = le.fit_transform(df['emotion'])\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        features, labels, stratify=labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    train_ds = LandmarkDataset(X_train, y_train)\n",
    "    val_ds = LandmarkDataset(X_val, y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=32)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = EmotionTransformer().to(device)\n",
    "    train_model(model, train_loader, val_loader, device)\n",
    "\n",
    "    print(\"ðŸŽ‰ Training completed. Model ready!\")\n",
    "    return model, le\n",
    "\n",
    "# ---------- Run ----------\n",
    "if __name__ == \"__main__\":\n",
    "    model, le = main(r\"C:\\Users\\Sneha\\Downloads\\JoyVerseDataSet.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6f6347-a6a6-48df-bc58-9613aee508b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4ad7ff-5d4d-4f94-a2f3-9307a0650da0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
